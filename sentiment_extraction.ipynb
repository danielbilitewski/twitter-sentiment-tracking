{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- average_polarity: string (nullable = true)\n",
      " |-- average_subjectivity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS NOTEBOOK DOES NOT DO ANY TEXT PRE-PROCESSING, SO THE EXTRACTED POLARITY AND SUBJECTIVITY IS NOT REALIABLE\n",
    "# THE PURPOSE OF THIS APPLICATION IS TO PROTOTYPE HOW A REAL-TIME SENTIMENT EXTRACTION CAN LOOK LIKE.\n",
    "\n",
    "# create environment variables for spark-submit\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 pyspark-shell'\n",
    "\n",
    "# create variables\n",
    "KAFKA_TOPIC_READ = 'texts_covid19'\n",
    "KAFKA_TOPIC_WRITE = 'sentiments_covid19'\n",
    "CHECKPOINT = 'checkpoint'\n",
    "\n",
    "# import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.sql.functions import udf, window, mean, first, max, round\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# create functions\n",
    "text_blob_udf = udf(lambda x: TextBlob(x)) # creates a TextBlob object\n",
    "polarity_udf = udf(lambda x: x.polarity, DoubleType()) # extracts polarity from TextBlob object\n",
    "subjectivity_udf = udf(lambda x: x.subjectivity, DoubleType()) # extracts subjectivity from TextBlob object\n",
    "double_to_string = udf(lambda x: str(x)[0:5]) # converts number in double format into string format with 3 decimal places\n",
    "\n",
    "# create spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# start a stream from a Kafka topic\n",
    "tweetsDF = spark.readStream\\\n",
    "                .format('kafka')\\\n",
    "                .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "                .option('subscribe', KAFKA_TOPIC_READ)\\\n",
    "                .load()\\\n",
    "                .withWatermark('timestamp', \"1 minutes\")\n",
    "\n",
    "# print dataframe schema\n",
    "tweetsDF.printSchema()\n",
    "\n",
    "# keep key, value and timestamp\n",
    "tweetsDF = tweetsDF.selectExpr(\"CAST(key AS STRING)\", \n",
    "                               \"CAST(value AS STRING)\",\n",
    "                               'timestamp')\n",
    "\n",
    "# print schema again to check the correct columns are kept\n",
    "tweetsDF.printSchema()\n",
    "\n",
    "# create a TextBlob object\n",
    "tweetsDF = tweetsDF.withColumn('textBlob', text_blob_udf(tweetsDF.value))\n",
    "\n",
    "# extract the polarity from the TextBlob object (-1 = negative, 1 = positive)\n",
    "# scale polarity to range [0,1]\n",
    "tweetsDF = tweetsDF.withColumn('polarity', (polarity_udf(tweetsDF.textBlob) + 1) / 2)\n",
    "\n",
    "# extract the subjectivity from the TextBlob object (0 = objective, 1 = subjective)\n",
    "tweetsDF = tweetsDF.withColumn('subjectivity', subjectivity_udf(tweetsDF.textBlob))\n",
    "\n",
    "# drop the text of the tweet and TextBlob object\n",
    "tweetsDF = tweetsDF.drop('value', 'textBlob')\n",
    "\n",
    "# aggregate by minute to get the average polarity and average subjectivity\n",
    "tweetsDF = tweetsDF.groupBy(window(tweetsDF.timestamp, \"1 minutes\", \"1 minutes\"))\\\n",
    "                   .agg(max('timestamp').alias('timestamp'),\n",
    "                        first('key').alias('key'),\n",
    "                        mean('polarity').alias('average_polarity'),\n",
    "                        mean('subjectivity').alias('average_subjectivity'))\n",
    "\n",
    "# make sure numbers have 3 digits\n",
    "tweetsDF = tweetsDF.withColumn('average_polarity', double_to_string(tweetsDF.average_polarity))\\\n",
    "                   .withColumn('average_subjectivity', double_to_string(tweetsDF.average_subjectivity))\n",
    "\n",
    "# print new schema of the dataframe\n",
    "tweetsDF.printSchema()\n",
    "\n",
    "# send average polarity and average subjectivity to a Kafka topic\n",
    "tweetStream = tweetsDF\\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"to_json(struct(*)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', KAFKA_TOPIC_WRITE)\\\n",
    "    .option('checkpointLocation', CHECKPOINT)\\\n",
    "    .start()\n",
    "\n",
    "tweetStream.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
